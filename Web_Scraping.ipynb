{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73bd6834",
   "metadata": {},
   "source": [
    "# Bfore You Execute this Notebook \n",
    "you have to install those packages  beautifulsoup4 , requests ,  pandas \n",
    "\n",
    "#### if not installed use this instruction :\n",
    "\n",
    "!pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c69316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f69e05",
   "metadata": {},
   "source": [
    "### import requests: \n",
    "This imports the requests library, which is used to make HTTP requests (like accessing a web page).\n",
    "### from bs4 import BeautifulSoup:\n",
    "BeautifulSoup is a Python library that helps to parse HTML and extract data from it. This line imports it to parse the webpage's content.\n",
    "### import pandas as pd:\n",
    "Pandas is a powerful library for data manipulation. We'll use it to store and process the data we scrape.\n",
    "### import os:\n",
    "This imports the os module, which helps in interacting with the operating system, like creating directories and managing files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d1889066",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.aduana.cl/exportacion-por-pais-y-codigo-arancelario/aduana/2018-12-14/101258.html'\n",
    "file_type = '.xlsx'  # File type to look for"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c929af8b",
   "metadata": {},
   "source": [
    "### url: \n",
    "This is the webpage we want to scrape. It contains links to Excel files that we are going to download.\n",
    "### file_type = '.xlsx': \n",
    "This variable defines the type of file we are looking for on the webpage. In this case, we are specifically interested in .xlsx files (Excel spreadsheets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4fa38ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = ['2020', '2021', '2022','2023', '2024']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d88a8b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_files = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ccce94",
   "metadata": {},
   "source": [
    "### years = ['2020', '2021', '2022', '2023', '2024']: \n",
    "This is a list of the specific years we are interested in. When scraping, we will look for links that contain these years (e.g., a link with 2020.xlsx).\n",
    "### downloaded_files = []: \n",
    "This creates an empty list that will be used to store the file paths of the downloaded Excel files. After each file is downloaded, its path will be added to this list for further processing later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d626eac9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khalil\\anaconda3\\envs\\tf\\lib\\site-packages\\urllib3\\connectionpool.py:1056: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.aduana.cl'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(url, verify=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973e8d8f",
   "metadata": {},
   "source": [
    "## The requests.get() \n",
    "function is used to retrieve the content of the web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6662126e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khalil\\anaconda3\\envs\\tf\\lib\\site-packages\\urllib3\\connectionpool.py:1056: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.aduana.cl'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded and saved: ./downloads\\2024.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khalil\\anaconda3\\envs\\tf\\lib\\site-packages\\urllib3\\connectionpool.py:1056: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.aduana.cl'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded and saved: ./downloads\\2023.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khalil\\anaconda3\\envs\\tf\\lib\\site-packages\\urllib3\\connectionpool.py:1056: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.aduana.cl'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded and saved: ./downloads\\2022.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khalil\\anaconda3\\envs\\tf\\lib\\site-packages\\urllib3\\connectionpool.py:1056: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.aduana.cl'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded and saved: ./downloads\\2021.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khalil\\anaconda3\\envs\\tf\\lib\\site-packages\\urllib3\\connectionpool.py:1056: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.aduana.cl'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded and saved: ./downloads\\2020.xlsx\n",
      "Downloaded files: ['./downloads\\\\2024.xlsx', './downloads\\\\2023.xlsx', './downloads\\\\2022.xlsx', './downloads\\\\2021.xlsx', './downloads\\\\2020.xlsx']\n"
     ]
    }
   ],
   "source": [
    "# List to store paths of downloaded files\n",
    "downloaded_files = []\n",
    "\n",
    "# Directory to save the files in the current directory of the Jupyter Notebook\n",
    "save_directory = './downloads'\n",
    "\n",
    "# Make sure the directory exists\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Make a request to the URL\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find and iterate over all <a> tags on the page\n",
    "    for link in soup.find_all('a'):\n",
    "        file_link = link.get('href')  # Get the href attribute from the <a> tag\n",
    "        if file_link and file_type in file_link:  # Check if the href contains '.xlsx'\n",
    "            full_url = file_link if file_link.startswith('http') else 'https://www.aduana.cl' + file_link  # Handle relative links\n",
    "            \n",
    "            # Check if the link contains the word \"peso\" and any of the specified years\n",
    "            if 'expo_pais_sa_peso' in full_url and any(year in full_url for year in years):\n",
    "                # Extract the year from the link\n",
    "                year = next((y for y in years if y in full_url), 'unknown')\n",
    "                \n",
    "                # Define the filename based on the year\n",
    "                filename = f'{year}.xlsx'\n",
    "                \n",
    "                # Define the full path for saving the file\n",
    "                file_path = os.path.join(save_directory, filename)\n",
    "                \n",
    "                # Download the file and save it locally\n",
    "                response = requests.get(full_url, verify=False)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    with open(file_path, 'wb') as file:\n",
    "                        file.write(response.content)\n",
    "                    \n",
    "                    # Add the file path to the list\n",
    "                    downloaded_files.append(file_path)\n",
    "                    \n",
    "                    print(f\"Downloaded and saved: {file_path}\")\n",
    "                else:\n",
    "                    print(f\"Failed to download: {full_url}\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "\n",
    "# Print the list of downloaded file paths\n",
    "print(\"Downloaded files:\", downloaded_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2fca96",
   "metadata": {},
   "source": [
    "In this part of the web scraping process, \n",
    "### 1 \n",
    "I started by initializing a list called downloaded_files to store the paths of the Excel files I would download. I also created a directory named './downloads' to save the files locally, and I used os.makedirs() to ensure the directory existed before downloading anything.\n",
    "\n",
    "### 2\n",
    "Next, I made a request to the specified URL, and if the request was successful (checked using response.status_code == 200), I used BeautifulSoup to parse the HTML content. I then iterated through all the '<a>' tags to find the relevant links. To filter out the correct Excel files, I looked for links containing the keyword 'expo_pais_sa_peso', which refers to weight in kilograms (\"peso\"). I also filtered the links based on the specific years I was interested  in (2020, 2021, 2022, 2024), ensuring I only downloaded the files for those years.\n",
    " \n",
    "### 3\n",
    "    \n",
    "For each valid link, I generated the full URL and downloaded the corresponding Excel file, saving it to the designated directory. After saving each file, I added its path to the downloaded_files list. This approach helped me target and download only the necessary Excel files from the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ac5a5147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved and moved to C:\\Users\\khalil\\Desktop\\Final_Scrapping_Result.xlsx\n"
     ]
    }
   ],
   "source": [
    "    # List of downloaded files\n",
    "    downloaded_files = [\n",
    "        'downloads/2020.xlsx',\n",
    "        'downloads/2021.xlsx',\n",
    "        'downloads/2022.xlsx',\n",
    "        'downloads/2023.xlsx',\n",
    "        'downloads/2024.xlsx'\n",
    "    ]\n",
    "\n",
    "    # List to store final DataFrame results\n",
    "    all_dfs = []\n",
    "\n",
    "    # Process each downloaded Excel file\n",
    "    for file_path in downloaded_files:\n",
    "        # Extract the year from the file path\n",
    "        year = os.path.basename(file_path).split('.')[0]\n",
    "\n",
    "        # Read the Excel file\n",
    "        df = pd.read_excel(file_path)\n",
    "\n",
    "        # Data cleaning and transformation\n",
    "        df = df.drop(df.columns[0], axis=1)\n",
    "        df = df.iloc[2:]\n",
    "        df.columns = df.iloc[0]\n",
    "        df = df[1:]  # Remove the first row (now the header)\n",
    "        df = df.drop(df.columns[-1], axis=1)\n",
    "\n",
    "        # Add 'city' column\n",
    "        df['city'] = pd.NA\n",
    "\n",
    "        # Initialize a variable to keep track of the current country name\n",
    "        current_city = None\n",
    "\n",
    "        # Iterate over the DataFrame rows\n",
    "        for index, row in df.iterrows():\n",
    "            value = row['País / Código Arancelario']\n",
    "\n",
    "            # Check if the value is a country name or code\n",
    "            if re.match(r'\\d', str(value)):  # If the value starts with a digit, it's a code\n",
    "                df.at[index, 'city'] = current_city\n",
    "            else:  # Otherwise, it's a country name\n",
    "                current_city = value\n",
    "                df.at[index, 'city'] = current_city\n",
    "\n",
    "        # Filter rows based on code\n",
    "        df = df[df['País / Código Arancelario'].str.startswith(('1001', '1005'))]\n",
    "\n",
    "        rows = []\n",
    "\n",
    "        # Process each row in the DataFrame\n",
    "        for _, row in df.iterrows():\n",
    "            code = row['País / Código Arancelario']\n",
    "\n",
    "            # Iterate over the columns excluding the first column\n",
    "            for col in df.columns[1:]:\n",
    "                date_str = col.split(' - ')[0]\n",
    "                value = row[col]\n",
    "\n",
    "                # Create the date in 'YYYY-MM-DD' format\n",
    "                month_number = {\n",
    "                    'Enero': '01',\n",
    "                    'Febrero': '02',\n",
    "                    'Marzo': '03',\n",
    "                    'Abril': '04',\n",
    "                    'Mayo': '05',\n",
    "                    'Junio': '06',\n",
    "                    'Julio': '07',\n",
    "                    'Agosto': '08',\n",
    "                    'Septiembre': '09',\n",
    "                    'Octubre': '10',\n",
    "                    'Noviembre': '11',\n",
    "                    'Diciembre': '12'\n",
    "                }.get(date_str, '00')  # Default to '00' if month is not found\n",
    "                date_formatted = f'{year}-{month_number}-01'\n",
    "\n",
    "                # Append the row to the list\n",
    "                rows.append({'code': code, 'date': date_formatted, 'value': value, 'city': row['city']})\n",
    "\n",
    "        # Convert the list of rows to a DataFrame\n",
    "        df1 = pd.DataFrame(rows)\n",
    "\n",
    "        # Convert 'date' column to datetime\n",
    "        df1['date'] = pd.to_datetime(df1['date'], format='%Y-%m-%d', errors='coerce')\n",
    "\n",
    "        # Filter out rows where 'value' is not a number\n",
    "        df1 = df1[pd.to_numeric(df1['value'], errors='coerce').notna()]\n",
    "\n",
    "        # Optionally, sort by 'code' and 'date'\n",
    "        df1 = df1.sort_values(by=['code', 'date']).reset_index(drop=True)\n",
    "\n",
    "        # Append the result to the list of DataFrames\n",
    "        all_dfs.append(df1) \n",
    "\n",
    "    # Combine all DataFrames into a single DataFrame\n",
    "    final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "    # Save the combined DataFrame to an Excel file\n",
    "    output_file_path = 'Final_Scrapping_Result.xlsx'\n",
    "    final_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "    # Define the path to your desktop\n",
    "    desktop_path = os.path.join(os.path.expanduser('~'), 'Desktop')\n",
    "\n",
    "    # Define the destination file path on the desktop\n",
    "    destination_file_path = os.path.join(desktop_path, 'Final_Scrapping_Result.xlsx')\n",
    "\n",
    "    # Move or rename the file to the desktop\n",
    "    os.rename(output_file_path, destination_file_path)\n",
    "\n",
    "    print(f'File saved and moved to {destination_file_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c188f48",
   "metadata": {},
   "source": [
    "In this code, I processed and combined multiple Excel files into a single consolidated DataFrame and then saved it to an Excel file on my desktop. Here's a step-by-step explanation:\n",
    "\n",
    "### List of Downloaded Files: \n",
    "I defined a list called downloaded_files containing the paths to the Excel files I previously downloaded.\n",
    "\n",
    "### Initialize List for DataFrames: \n",
    "An empty list named all_dfs was created to store DataFrames for each Excel file after processing.\n",
    "\n",
    "### Process Each Excel File: \n",
    "For each file in the downloaded_files list:\n",
    "\n",
    "### Extract Year: \n",
    "The year is extracted from the file name by splitting the file path and getting the base name.\n",
    "### Read File: \n",
    "The Excel file is read into a DataFrame using pd.read_excel().\n",
    "## Data Cleaning and Transformation:\n",
    "The first column is dropped.\n",
    "The DataFrame is sliced to start from the third row.\n",
    "The first row is set as the header.\n",
    "The last column is removed.\n",
    "\n",
    "### Add 'City' Column:\n",
    "A new column named 'city' is added to keep track of the city names.\n",
    "Iterate over each row to determine if the row represents a country or a code.\n",
    "If it’s a country name, it’s used to update the 'city' column for subsequent rows with codes.\n",
    "### Filter Rows: \n",
    "Rows are filtered based on whether the 'País / Código Arancelario' column starts with '1001' or '1005'.\n",
    "### Process Each Row:\n",
    "For each row, iterate over columns to extract and format dates.\n",
    "Convert month names to month numbers and format the date based on the extracted year.\n",
    "Append relevant data (code, formatted date, value, city) to a list.\n",
    "### Create DataFrame: \n",
    "Convert the list of rows into a new DataFrame, df1.\n",
    "### Convert and Filter Dates: \n",
    "Convert the 'date' column to datetime format and filter out rows where 'value' is not a number.\n",
    "### Sort and Reset Index: \n",
    "Sort the DataFrame by 'code' and 'date', and reset the index.\n",
    "### Combine DataFrames: \n",
    "All individual DataFrames from the all_dfs list are concatenated into a single DataFrame, final_df.\n",
    "\n",
    "## Save and Move Final DataFrame:\n",
    "\n",
    "### Save to Excel: \n",
    "The combined DataFrame is saved to an Excel file named ## Final_Scrapping_Result.xlsx ##.\n",
    "### Move to Desktop: \n",
    "The file is then moved to the desktop directory for easy access.\n",
    "### Print Confirmation: \n",
    "A message is printed to confirm that the file has been saved and moved to the desktop.\n",
    "\n",
    "This process ensures that all relevant data from multiple Excel files is consolidated into a single, organized file for further analysis or reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41382b04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
